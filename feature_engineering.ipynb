{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s_qgjPpI5s_i"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.svm import SVC\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwEcs6at5s_l",
        "outputId": "94c04d80-4ebe-4373-fb8a-a56b1ac324c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: C:\\Users\\umbeg\\.cache\\kagglehub\\datasets\\sooyoungher\\smoking-drinking-dataset\\versions\\2\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sooyoungher/smoking-drinking-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "file_name = \"smoking_driking_dataset_Ver01.csv\"\n",
        "file_path = f\"{path}/{file_name}\"\n",
        "dataset = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save 10% of the dataset to a file\n",
        "file_name = \"smoking_driking_dataset_Ver01_10percent.csv\"\n",
        "file_path = f\"{path}/{file_name}\"\n",
        "\n",
        "dataset.sample(frac=0.1).to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sex', 'age', 'height', 'weight', 'waistline', 'sight_left',\n",
              "       'sight_right', 'hear_left', 'hear_right', 'SBP', 'DBP', 'BLDS',\n",
              "       'tot_chole', 'HDL_chole', 'LDL_chole', 'triglyceride', 'hemoglobin',\n",
              "       'urine_protein', 'serum_creatinine', 'SGOT_AST', 'SGOT_ALT',\n",
              "       'gamma_GTP', 'SMK_stat_type_cd', 'DRK_YN'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3CZcARzzNPCJ"
      },
      "outputs": [],
      "source": [
        "# Encode categorical features (if any)\n",
        "label_encoders = {}\n",
        "\n",
        "for column in dataset.columns:\n",
        "    if dataset[column].dtype == 'object':\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        dataset[column] = label_encoders[column].fit_transform(dataset[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "k5v7vwzcNXki",
        "outputId": "04c6b11f-a6c0-4b51-e95e-73622aa98c44"
      },
      "outputs": [],
      "source": [
        "# Feature engineering\n",
        "\n",
        "dataset['BMI'] = dataset['weight'] / (dataset['height'] / 100) ** 2                     # Use to assess overall obesity.\n",
        "dataset['wth_ratio'] = dataset['waistline'] / dataset['height']                         # A good indicator of central (abdominal) obesity and cardiovascular risk.\n",
        "dataset['wtw_ratio'] = dataset['waistline'] / dataset['weight']                         # May provide additional insights into fat distribution.\n",
        "dataset['pulse_pressure'] = dataset['SBP'] - dataset['DBP']                             # High pulse pressure is linked to arterial stiffness and cardiovascular risk.\n",
        "dataset['mean_arterial_pressure'] = dataset['DBP'] + (dataset['pulse_pressure'] / 3)    # Represents the average blood pressure during a single cardiac cycle.\n",
        "dataset['TC_HDL_ratio'] = dataset['tot_chole'] / dataset['HDL_chole']                   # Widely used to assess cardiovascular risk.\n",
        "dataset['LDL_HDL_ratio'] = dataset['LDL_chole'] / dataset['HDL_chole']                              \n",
        "dataset['non_HDL_chole'] = dataset['tot_chole'] - dataset['HDL_chole']                  # Represents the atherogenic (bad) cholesterol particles.\n",
        "dataset['AST_ALT_ratio'] = dataset['SGOT_AST'] / dataset['SGOT_ALT']                    # Can help in differentiating types of liver disease (for example, a high ratio might indicate alcoholic liver disease)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>height</th>\n",
              "      <th>weight</th>\n",
              "      <th>waistline</th>\n",
              "      <th>sight_left</th>\n",
              "      <th>sight_right</th>\n",
              "      <th>hear_left</th>\n",
              "      <th>hear_right</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>BLDS</th>\n",
              "      <th>tot_chole</th>\n",
              "      <th>HDL_chole</th>\n",
              "      <th>LDL_chole</th>\n",
              "      <th>triglyceride</th>\n",
              "      <th>hemoglobin</th>\n",
              "      <th>urine_protein</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>SGOT_AST</th>\n",
              "      <th>SGOT_ALT</th>\n",
              "      <th>gamma_GTP</th>\n",
              "      <th>SMK_stat_type_cd</th>\n",
              "      <th>DRK_YN</th>\n",
              "      <th>BMI</th>\n",
              "      <th>wth_ratio</th>\n",
              "      <th>wtw_ratio</th>\n",
              "      <th>pulse_pressure</th>\n",
              "      <th>mean_arterial_pressure</th>\n",
              "      <th>TC_HDL_ratio</th>\n",
              "      <th>LDL_HDL_ratio</th>\n",
              "      <th>non_HDL_chole</th>\n",
              "      <th>AST_ALT_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35</td>\n",
              "      <td>170</td>\n",
              "      <td>75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>17.1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>25.951557</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>1.200000</td>\n",
              "      <td>40.0</td>\n",
              "      <td>93.333333</td>\n",
              "      <td>4.020833</td>\n",
              "      <td>2.625000</td>\n",
              "      <td>145.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>30</td>\n",
              "      <td>180</td>\n",
              "      <td>80</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>228.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>20.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>24.691358</td>\n",
              "      <td>0.494444</td>\n",
              "      <td>1.112500</td>\n",
              "      <td>48.0</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>4.145455</td>\n",
              "      <td>2.690909</td>\n",
              "      <td>173.0</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>165</td>\n",
              "      <td>75</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>15.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>47.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>27.548209</td>\n",
              "      <td>0.551515</td>\n",
              "      <td>1.213333</td>\n",
              "      <td>50.0</td>\n",
              "      <td>86.666667</td>\n",
              "      <td>3.317073</td>\n",
              "      <td>1.804878</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.468750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>175</td>\n",
              "      <td>80</td>\n",
              "      <td>91.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>17.6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>26.122449</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>1.137500</td>\n",
              "      <td>58.0</td>\n",
              "      <td>106.333333</td>\n",
              "      <td>2.644737</td>\n",
              "      <td>1.368421</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0.852941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>165</td>\n",
              "      <td>60</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>199.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>19.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>22.038567</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>1.333333</td>\n",
              "      <td>56.0</td>\n",
              "      <td>100.666667</td>\n",
              "      <td>3.262295</td>\n",
              "      <td>1.918033</td>\n",
              "      <td>138.0</td>\n",
              "      <td>1.583333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sex  age  height  weight  ...  TC_HDL_ratio  LDL_HDL_ratio  non_HDL_chole  AST_ALT_ratio\n",
              "0    1   35     170      75  ...      4.020833       2.625000          145.0       0.600000\n",
              "1    1   30     180      80  ...      4.145455       2.690909          173.0       0.555556\n",
              "2    1   40     165      75  ...      3.317073       1.804878           95.0       1.468750\n",
              "3    1   50     175      80  ...      2.644737       1.368421          125.0       0.852941\n",
              "4    1   50     165      60  ...      3.262295       1.918033          138.0       1.583333\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dataset.drop(['DRK_YN', 'SMK_stat_type_cd'], axis=1).copy()\n",
        "y_drink = dataset['DRK_YN'].copy()\n",
        "y_smoke = dataset['SMK_stat_type_cd'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VfiFgix99tgL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature importances for DRK_YN:\n",
            "                   Feature  Importance\n",
            "21               gamma_GTP    0.070685\n",
            "0                      sex    0.064416\n",
            "1                      age    0.060693\n",
            "16              hemoglobin    0.056116\n",
            "24               wtw_ratio    0.045835\n",
            "30           AST_ALT_ratio    0.042561\n",
            "28           LDL_HDL_ratio    0.040513\n",
            "27            TC_HDL_ratio    0.039841\n",
            "15            triglyceride    0.038596\n",
            "13               HDL_chole    0.036026\n",
            "2                   height    0.035320\n",
            "11                    BLDS    0.034155\n",
            "23               wth_ratio    0.032974\n",
            "12               tot_chole    0.031692\n",
            "20                SGOT_ALT    0.030980\n",
            "26  mean_arterial_pressure    0.030761\n",
            "14               LDL_chole    0.030571\n",
            "29           non_HDL_chole    0.029757\n",
            "4                waistline    0.028910\n",
            "25          pulse_pressure    0.028535\n",
            "19                SGOT_AST    0.028261\n",
            "9                      SBP    0.027647\n",
            "10                     DBP    0.026264\n",
            "18        serum_creatinine    0.021770\n",
            "5               sight_left    0.020864\n",
            "6              sight_right    0.020816\n",
            "22                     BMI    0.020545\n",
            "3                   weight    0.018800\n",
            "17           urine_protein    0.003462\n",
            "7                hear_left    0.001337\n",
            "8               hear_right    0.001295\n"
          ]
        }
      ],
      "source": [
        "# Initialize and train a RandomForestClassifier for drinking\n",
        "rf_drink = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_drink.fit(X, y_drink)\n",
        "\n",
        "# Get feature importances for drinking\n",
        "feature_importances_drink = rf_drink.feature_importances_\n",
        "\n",
        "feature_importance_df_drink = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_drink\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature importances for DRK_YN:\")\n",
        "print(feature_importance_df_drink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize and train a RandomForestClassifier for smoking\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rf_smoke \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m rf_smoke\u001b[38;5;241m.\u001b[39mfit(X, y_smoke)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get feature importances for smoking\u001b[39;00m\n\u001b[0;32m      6\u001b[0m feature_importances_smoke \u001b[38;5;241m=\u001b[39m rf_smoke\u001b[38;5;241m.\u001b[39mfeature_importances_\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    488\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    489\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    490\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    491\u001b[0m )(\n\u001b[0;32m    492\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    493\u001b[0m         t,\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    495\u001b[0m         X,\n\u001b[0;32m    496\u001b[0m         y,\n\u001b[0;32m    497\u001b[0m         sample_weight,\n\u001b[0;32m    498\u001b[0m         i,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    500\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    501\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    502\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    503\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    504\u001b[0m     )\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "y_drink_pred = rf_drink.predict(X)\n",
        "report = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train a RandomForestClassifier for smoking\n",
        "rf_smoke = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_smoke.fit(X, y_smoke)\n",
        "\n",
        "# Get feature importances for smoking\n",
        "feature_importances_smoke = rf_smoke.feature_importances_\n",
        "\n",
        "feature_importance_df_smoke = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_smoke\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature importances for SMK_stat_type_cd:\")\n",
        "print(feature_importance_df_smoke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Initialize and train an AdaBoostClassifier for drinking\n",
        "ab_drink = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "ab_drink.fit(X, y_drink)\n",
        "\n",
        "# Get feature importances for drinking\n",
        "feature_importances_drink = ab_drink.feature_importances_\n",
        "\n",
        "feature_importance_df_drink = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_drink\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature importances for DRK_YN:\")\n",
        "print(feature_importance_df_drink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train an AdaBoostClassifier for smoking\n",
        "ab_smoke = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "ab_smoke.fit(X, y_smoke)\n",
        "\n",
        "# Get feature importances for smoking\n",
        "feature_importances_smoke = ab_smoke.feature_importances_\n",
        "\n",
        "feature_importance_df_smoke = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_smoke\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature importances for SMK_stat_type_cd:\")\n",
        "print(feature_importance_df_smoke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = X.select_dtypes(include=['number']).columns\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_smoke, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original feature count: 22\n",
            "Reduced feature count: 15\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=0.90)  # Retain 95% of the variance\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(f\"Original feature count: {X_train.shape[1]}\")\n",
        "print(f\"Reduced feature count: {X_train_pca.shape[1]}\")\n",
        "\n",
        "X_train = X_train_pca\n",
        "X_test = X_test_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                                          | 0/1 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define parameter grid\n",
        "parameter_grid = ParameterGrid({'n_neighbors': [100,]})\n",
        "\n",
        "# Initialize variables for best model\n",
        "best_knn = None\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "# Perform manual grid search\n",
        "for params in tqdm(parameter_grid):\n",
        "    knn = KNeighborsClassifier(n_jobs=-1, **params)\n",
        "    knn.fit(X_train, y_train)\n",
        "    score = knn.score(X_test, y_test)\n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_knn = knn\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best number of neighbors: {best_params['n_neighbors']}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "# Generate a classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
        "\n",
        "X = X_poly_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mgiacopu/anaconda3/envs/data-mining/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.72      0.73     99595\n",
            "           1       0.72      0.75      0.73     98675\n",
            "\n",
            "    accuracy                           0.73    198270\n",
            "   macro avg       0.73      0.73      0.73    198270\n",
            "weighted avg       0.73      0.73      0.73    198270\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X_train, X_test, y_drink_train, y_drink_test = train_test_split(X, y_drink, test_size=0.2, random_state=42)\n",
        "\n",
        "drink_clf = AdaBoostClassifier(n_estimators=200, random_state=42)\n",
        "drink_clf.fit(X_train, y_drink_train)\n",
        "y_drink_pred = drink_clf.predict(X_test)\n",
        "\n",
        "report = classification_report(y_drink_test, y_drink_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DlyXaXFB-gd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: HDL_chole gamma_GTP, Importance: 0.11\n",
            "Feature: sex hemoglobin, Importance: 0.075\n",
            "Feature: age SGOT_ALT, Importance: 0.055\n",
            "Feature: sex age, Importance: 0.045\n",
            "Feature: LDL_chole SGOT_ALT, Importance: 0.045\n",
            "Feature: sex gamma_GTP, Importance: 0.04\n",
            "Feature: age^2, Importance: 0.04\n",
            "Feature: height HDL_chole, Importance: 0.035\n",
            "Feature: age, Importance: 0.025\n",
            "Feature: sex SGOT_ALT, Importance: 0.02\n",
            "Feature: age height, Importance: 0.02\n",
            "Feature: HDL_chole SGOT_AST, Importance: 0.02\n",
            "Feature: triglyceride SGOT_AST, Importance: 0.02\n",
            "Feature: triglyceride gamma_GTP, Importance: 0.02\n",
            "Feature: serum_creatinine SGOT_ALT, Importance: 0.02\n",
            "Feature: height, Importance: 0.015\n",
            "Feature: gamma_GTP, Importance: 0.015\n",
            "Feature: sex triglyceride, Importance: 0.015\n",
            "Feature: age hear_right, Importance: 0.015\n",
            "Feature: age LDL_chole, Importance: 0.015\n",
            "Feature: age serum_creatinine, Importance: 0.015\n",
            "Feature: HDL_chole SGOT_ALT, Importance: 0.015\n",
            "Feature: DBP, Importance: 0.01\n",
            "Feature: tot_chole, Importance: 0.01\n",
            "Feature: sex waistline, Importance: 0.01\n",
            "Feature: sex SGOT_AST, Importance: 0.01\n",
            "Feature: age triglyceride, Importance: 0.01\n",
            "Feature: age hemoglobin, Importance: 0.01\n",
            "Feature: height SGOT_ALT, Importance: 0.01\n",
            "Feature: weight SGOT_AST, Importance: 0.01\n",
            "Feature: waistline^2, Importance: 0.01\n",
            "Feature: hear_left gamma_GTP, Importance: 0.01\n",
            "Feature: DBP HDL_chole, Importance: 0.01\n",
            "Feature: BLDS SGOT_AST, Importance: 0.01\n",
            "Feature: HDL_chole hemoglobin, Importance: 0.01\n",
            "Feature: hemoglobin, Importance: 0.005\n",
            "Feature: SGOT_ALT, Importance: 0.005\n",
            "Feature: sex height, Importance: 0.005\n",
            "Feature: sex SBP, Importance: 0.005\n",
            "Feature: sex DBP, Importance: 0.005\n",
            "Feature: sex tot_chole, Importance: 0.005\n",
            "Feature: age weight, Importance: 0.005\n",
            "Feature: age DBP, Importance: 0.005\n",
            "Feature: age tot_chole, Importance: 0.005\n",
            "Feature: age urine_protein, Importance: 0.005\n",
            "Feature: age gamma_GTP, Importance: 0.005\n",
            "Feature: height hear_right, Importance: 0.005\n",
            "Feature: weight waistline, Importance: 0.005\n",
            "Feature: weight BLDS, Importance: 0.005\n",
            "Feature: weight hemoglobin, Importance: 0.005\n",
            "Feature: waistline BLDS, Importance: 0.005\n",
            "Feature: waistline HDL_chole, Importance: 0.005\n",
            "Feature: waistline serum_creatinine, Importance: 0.005\n",
            "Feature: sight_right HDL_chole, Importance: 0.005\n",
            "Feature: sight_right serum_creatinine, Importance: 0.005\n",
            "Feature: hear_left SBP, Importance: 0.005\n",
            "Feature: SBP urine_protein, Importance: 0.005\n",
            "Feature: SBP serum_creatinine, Importance: 0.005\n",
            "Feature: SBP SGOT_AST, Importance: 0.005\n",
            "Feature: DBP^2, Importance: 0.005\n",
            "Feature: BLDS^2, Importance: 0.005\n",
            "Feature: BLDS urine_protein, Importance: 0.005\n",
            "Feature: tot_chole^2, Importance: 0.005\n",
            "Feature: tot_chole HDL_chole, Importance: 0.005\n",
            "Feature: HDL_chole LDL_chole, Importance: 0.005\n",
            "Feature: LDL_chole SGOT_AST, Importance: 0.005\n",
            "Feature: LDL_chole gamma_GTP, Importance: 0.005\n",
            "Feature: urine_protein gamma_GTP, Importance: 0.005\n",
            "Feature: serum_creatinine^2, Importance: 0.005\n",
            "Feature: SGOT_AST gamma_GTP, Importance: 0.005\n",
            "Feature: sex, Importance: 0.0\n",
            "Feature: weight, Importance: 0.0\n",
            "Feature: waistline, Importance: 0.0\n",
            "Feature: sight_left, Importance: 0.0\n",
            "Feature: sight_right, Importance: 0.0\n",
            "Feature: hear_left, Importance: 0.0\n",
            "Feature: hear_right, Importance: 0.0\n",
            "Feature: SBP, Importance: 0.0\n",
            "Feature: BLDS, Importance: 0.0\n",
            "Feature: HDL_chole, Importance: 0.0\n",
            "Feature: LDL_chole, Importance: 0.0\n",
            "Feature: triglyceride, Importance: 0.0\n",
            "Feature: urine_protein, Importance: 0.0\n",
            "Feature: serum_creatinine, Importance: 0.0\n",
            "Feature: SGOT_AST, Importance: 0.0\n",
            "Feature: sex^2, Importance: 0.0\n",
            "Feature: sex weight, Importance: 0.0\n",
            "Feature: sex sight_left, Importance: 0.0\n",
            "Feature: sex sight_right, Importance: 0.0\n",
            "Feature: sex hear_left, Importance: 0.0\n",
            "Feature: sex hear_right, Importance: 0.0\n",
            "Feature: sex BLDS, Importance: 0.0\n",
            "Feature: sex HDL_chole, Importance: 0.0\n",
            "Feature: sex LDL_chole, Importance: 0.0\n",
            "Feature: sex urine_protein, Importance: 0.0\n",
            "Feature: sex serum_creatinine, Importance: 0.0\n",
            "Feature: age waistline, Importance: 0.0\n",
            "Feature: age sight_left, Importance: 0.0\n",
            "Feature: age sight_right, Importance: 0.0\n",
            "Feature: age hear_left, Importance: 0.0\n",
            "Feature: age SBP, Importance: 0.0\n",
            "Feature: age BLDS, Importance: 0.0\n",
            "Feature: age HDL_chole, Importance: 0.0\n",
            "Feature: age SGOT_AST, Importance: 0.0\n",
            "Feature: height^2, Importance: 0.0\n",
            "Feature: height weight, Importance: 0.0\n",
            "Feature: height waistline, Importance: 0.0\n",
            "Feature: height sight_left, Importance: 0.0\n",
            "Feature: height sight_right, Importance: 0.0\n",
            "Feature: height hear_left, Importance: 0.0\n",
            "Feature: height SBP, Importance: 0.0\n",
            "Feature: height DBP, Importance: 0.0\n",
            "Feature: height BLDS, Importance: 0.0\n",
            "Feature: height tot_chole, Importance: 0.0\n",
            "Feature: height LDL_chole, Importance: 0.0\n",
            "Feature: height triglyceride, Importance: 0.0\n",
            "Feature: height hemoglobin, Importance: 0.0\n",
            "Feature: height urine_protein, Importance: 0.0\n",
            "Feature: height serum_creatinine, Importance: 0.0\n",
            "Feature: height SGOT_AST, Importance: 0.0\n",
            "Feature: height gamma_GTP, Importance: 0.0\n",
            "Feature: weight^2, Importance: 0.0\n",
            "Feature: weight sight_left, Importance: 0.0\n",
            "Feature: weight sight_right, Importance: 0.0\n",
            "Feature: weight hear_left, Importance: 0.0\n",
            "Feature: weight hear_right, Importance: 0.0\n",
            "Feature: weight SBP, Importance: 0.0\n",
            "Feature: weight DBP, Importance: 0.0\n",
            "Feature: weight tot_chole, Importance: 0.0\n",
            "Feature: weight HDL_chole, Importance: 0.0\n",
            "Feature: weight LDL_chole, Importance: 0.0\n",
            "Feature: weight triglyceride, Importance: 0.0\n",
            "Feature: weight urine_protein, Importance: 0.0\n",
            "Feature: weight serum_creatinine, Importance: 0.0\n",
            "Feature: weight SGOT_ALT, Importance: 0.0\n",
            "Feature: weight gamma_GTP, Importance: 0.0\n",
            "Feature: waistline sight_left, Importance: 0.0\n",
            "Feature: waistline sight_right, Importance: 0.0\n",
            "Feature: waistline hear_left, Importance: 0.0\n",
            "Feature: waistline hear_right, Importance: 0.0\n",
            "Feature: waistline SBP, Importance: 0.0\n",
            "Feature: waistline DBP, Importance: 0.0\n",
            "Feature: waistline tot_chole, Importance: 0.0\n",
            "Feature: waistline LDL_chole, Importance: 0.0\n",
            "Feature: waistline triglyceride, Importance: 0.0\n",
            "Feature: waistline hemoglobin, Importance: 0.0\n",
            "Feature: waistline urine_protein, Importance: 0.0\n",
            "Feature: waistline SGOT_AST, Importance: 0.0\n",
            "Feature: waistline SGOT_ALT, Importance: 0.0\n",
            "Feature: waistline gamma_GTP, Importance: 0.0\n",
            "Feature: sight_left^2, Importance: 0.0\n",
            "Feature: sight_left sight_right, Importance: 0.0\n",
            "Feature: sight_left hear_left, Importance: 0.0\n",
            "Feature: sight_left hear_right, Importance: 0.0\n",
            "Feature: sight_left SBP, Importance: 0.0\n",
            "Feature: sight_left DBP, Importance: 0.0\n",
            "Feature: sight_left BLDS, Importance: 0.0\n",
            "Feature: sight_left tot_chole, Importance: 0.0\n",
            "Feature: sight_left HDL_chole, Importance: 0.0\n",
            "Feature: sight_left LDL_chole, Importance: 0.0\n",
            "Feature: sight_left triglyceride, Importance: 0.0\n",
            "Feature: sight_left hemoglobin, Importance: 0.0\n",
            "Feature: sight_left urine_protein, Importance: 0.0\n",
            "Feature: sight_left serum_creatinine, Importance: 0.0\n",
            "Feature: sight_left SGOT_AST, Importance: 0.0\n",
            "Feature: sight_left SGOT_ALT, Importance: 0.0\n",
            "Feature: sight_left gamma_GTP, Importance: 0.0\n",
            "Feature: sight_right^2, Importance: 0.0\n",
            "Feature: sight_right hear_left, Importance: 0.0\n",
            "Feature: sight_right hear_right, Importance: 0.0\n",
            "Feature: sight_right SBP, Importance: 0.0\n",
            "Feature: sight_right DBP, Importance: 0.0\n",
            "Feature: sight_right BLDS, Importance: 0.0\n",
            "Feature: sight_right tot_chole, Importance: 0.0\n",
            "Feature: sight_right LDL_chole, Importance: 0.0\n",
            "Feature: sight_right triglyceride, Importance: 0.0\n",
            "Feature: sight_right hemoglobin, Importance: 0.0\n",
            "Feature: sight_right urine_protein, Importance: 0.0\n",
            "Feature: sight_right SGOT_AST, Importance: 0.0\n",
            "Feature: sight_right SGOT_ALT, Importance: 0.0\n",
            "Feature: sight_right gamma_GTP, Importance: 0.0\n",
            "Feature: hear_left^2, Importance: 0.0\n",
            "Feature: hear_left hear_right, Importance: 0.0\n",
            "Feature: hear_left DBP, Importance: 0.0\n",
            "Feature: hear_left BLDS, Importance: 0.0\n",
            "Feature: hear_left tot_chole, Importance: 0.0\n",
            "Feature: hear_left HDL_chole, Importance: 0.0\n",
            "Feature: hear_left LDL_chole, Importance: 0.0\n",
            "Feature: hear_left triglyceride, Importance: 0.0\n",
            "Feature: hear_left hemoglobin, Importance: 0.0\n",
            "Feature: hear_left urine_protein, Importance: 0.0\n",
            "Feature: hear_left serum_creatinine, Importance: 0.0\n",
            "Feature: hear_left SGOT_AST, Importance: 0.0\n",
            "Feature: hear_left SGOT_ALT, Importance: 0.0\n",
            "Feature: hear_right^2, Importance: 0.0\n",
            "Feature: hear_right SBP, Importance: 0.0\n",
            "Feature: hear_right DBP, Importance: 0.0\n",
            "Feature: hear_right BLDS, Importance: 0.0\n",
            "Feature: hear_right tot_chole, Importance: 0.0\n",
            "Feature: hear_right HDL_chole, Importance: 0.0\n",
            "Feature: hear_right LDL_chole, Importance: 0.0\n",
            "Feature: hear_right triglyceride, Importance: 0.0\n",
            "Feature: hear_right hemoglobin, Importance: 0.0\n",
            "Feature: hear_right urine_protein, Importance: 0.0\n",
            "Feature: hear_right serum_creatinine, Importance: 0.0\n",
            "Feature: hear_right SGOT_AST, Importance: 0.0\n",
            "Feature: hear_right SGOT_ALT, Importance: 0.0\n",
            "Feature: hear_right gamma_GTP, Importance: 0.0\n",
            "Feature: SBP^2, Importance: 0.0\n",
            "Feature: SBP DBP, Importance: 0.0\n",
            "Feature: SBP BLDS, Importance: 0.0\n",
            "Feature: SBP tot_chole, Importance: 0.0\n",
            "Feature: SBP HDL_chole, Importance: 0.0\n",
            "Feature: SBP LDL_chole, Importance: 0.0\n",
            "Feature: SBP triglyceride, Importance: 0.0\n",
            "Feature: SBP hemoglobin, Importance: 0.0\n",
            "Feature: SBP SGOT_ALT, Importance: 0.0\n",
            "Feature: SBP gamma_GTP, Importance: 0.0\n",
            "Feature: DBP BLDS, Importance: 0.0\n",
            "Feature: DBP tot_chole, Importance: 0.0\n",
            "Feature: DBP LDL_chole, Importance: 0.0\n",
            "Feature: DBP triglyceride, Importance: 0.0\n",
            "Feature: DBP hemoglobin, Importance: 0.0\n",
            "Feature: DBP urine_protein, Importance: 0.0\n",
            "Feature: DBP serum_creatinine, Importance: 0.0\n",
            "Feature: DBP SGOT_AST, Importance: 0.0\n",
            "Feature: DBP SGOT_ALT, Importance: 0.0\n",
            "Feature: DBP gamma_GTP, Importance: 0.0\n",
            "Feature: BLDS tot_chole, Importance: 0.0\n",
            "Feature: BLDS HDL_chole, Importance: 0.0\n",
            "Feature: BLDS LDL_chole, Importance: 0.0\n",
            "Feature: BLDS triglyceride, Importance: 0.0\n",
            "Feature: BLDS hemoglobin, Importance: 0.0\n",
            "Feature: BLDS serum_creatinine, Importance: 0.0\n",
            "Feature: BLDS SGOT_ALT, Importance: 0.0\n",
            "Feature: BLDS gamma_GTP, Importance: 0.0\n",
            "Feature: tot_chole LDL_chole, Importance: 0.0\n",
            "Feature: tot_chole triglyceride, Importance: 0.0\n",
            "Feature: tot_chole hemoglobin, Importance: 0.0\n",
            "Feature: tot_chole urine_protein, Importance: 0.0\n",
            "Feature: tot_chole serum_creatinine, Importance: 0.0\n",
            "Feature: tot_chole SGOT_AST, Importance: 0.0\n",
            "Feature: tot_chole SGOT_ALT, Importance: 0.0\n",
            "Feature: tot_chole gamma_GTP, Importance: 0.0\n",
            "Feature: HDL_chole^2, Importance: 0.0\n",
            "Feature: HDL_chole triglyceride, Importance: 0.0\n",
            "Feature: HDL_chole urine_protein, Importance: 0.0\n",
            "Feature: HDL_chole serum_creatinine, Importance: 0.0\n",
            "Feature: LDL_chole^2, Importance: 0.0\n",
            "Feature: LDL_chole triglyceride, Importance: 0.0\n",
            "Feature: LDL_chole hemoglobin, Importance: 0.0\n",
            "Feature: LDL_chole urine_protein, Importance: 0.0\n",
            "Feature: LDL_chole serum_creatinine, Importance: 0.0\n",
            "Feature: triglyceride^2, Importance: 0.0\n",
            "Feature: triglyceride hemoglobin, Importance: 0.0\n",
            "Feature: triglyceride urine_protein, Importance: 0.0\n",
            "Feature: triglyceride serum_creatinine, Importance: 0.0\n",
            "Feature: triglyceride SGOT_ALT, Importance: 0.0\n",
            "Feature: hemoglobin^2, Importance: 0.0\n",
            "Feature: hemoglobin urine_protein, Importance: 0.0\n",
            "Feature: hemoglobin serum_creatinine, Importance: 0.0\n",
            "Feature: hemoglobin SGOT_AST, Importance: 0.0\n",
            "Feature: hemoglobin SGOT_ALT, Importance: 0.0\n",
            "Feature: hemoglobin gamma_GTP, Importance: 0.0\n",
            "Feature: urine_protein^2, Importance: 0.0\n",
            "Feature: urine_protein serum_creatinine, Importance: 0.0\n",
            "Feature: urine_protein SGOT_AST, Importance: 0.0\n",
            "Feature: urine_protein SGOT_ALT, Importance: 0.0\n",
            "Feature: serum_creatinine SGOT_AST, Importance: 0.0\n",
            "Feature: serum_creatinine gamma_GTP, Importance: 0.0\n",
            "Feature: SGOT_AST^2, Importance: 0.0\n",
            "Feature: SGOT_AST SGOT_ALT, Importance: 0.0\n",
            "Feature: SGOT_ALT^2, Importance: 0.0\n",
            "Feature: SGOT_ALT gamma_GTP, Importance: 0.0\n",
            "Feature: gamma_GTP^2, Importance: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Print or visualize feature importances for drinking\n",
        "for feature_name, importance in sorted(zip(X.columns, drink_clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"Feature: {feature_name}, Importance: {importance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = dataset.drop(['DRK_YN', 'SMK_stat_type_cd'], axis=1).copy()\n",
        "y_drink = dataset['DRK_YN'].copy()\n",
        "\n",
        "X['HDL_chole gamma_GTP'] = X['HDL_chole'] * X['gamma_GTP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mgiacopu/anaconda3/envs/data-mining/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.7232359913249609\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.72      0.72     99595\n",
            "           1       0.72      0.72      0.72     98675\n",
            "\n",
            "    accuracy                           0.72    198270\n",
            "   macro avg       0.72      0.72      0.72    198270\n",
            "weighted avg       0.72      0.72      0.72    198270\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_drink_train, y_drink_test = train_test_split(X, y_drink, test_size=0.2, random_state=42)\n",
        "\n",
        "drink_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "drink_clf.fit(X_train, y_drink_train)\n",
        "y_drink_pred = drink_clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy\", accuracy_score(y_drink_test, y_drink_pred))\n",
        "\n",
        "report = classification_report(y_drink_test, y_drink_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature: age, Importance: 0.24\n",
            "Feature: HDL_chole gamma_GTP, Importance: 0.22\n",
            "Feature: SGOT_ALT, Importance: 0.16\n",
            "Feature: HDL_chole, Importance: 0.08\n",
            "Feature: serum_creatinine, Importance: 0.06\n",
            "Feature: BLDS, Importance: 0.04\n",
            "Feature: LDL_chole, Importance: 0.04\n",
            "Feature: SGOT_AST, Importance: 0.04\n",
            "Feature: sex, Importance: 0.02\n",
            "Feature: height, Importance: 0.02\n",
            "Feature: weight, Importance: 0.02\n",
            "Feature: DBP, Importance: 0.02\n",
            "Feature: triglyceride, Importance: 0.02\n",
            "Feature: hemoglobin, Importance: 0.02\n",
            "Feature: waistline, Importance: 0.0\n",
            "Feature: sight_left, Importance: 0.0\n",
            "Feature: sight_right, Importance: 0.0\n",
            "Feature: hear_left, Importance: 0.0\n",
            "Feature: hear_right, Importance: 0.0\n",
            "Feature: SBP, Importance: 0.0\n",
            "Feature: tot_chole, Importance: 0.0\n",
            "Feature: urine_protein, Importance: 0.0\n",
            "Feature: gamma_GTP, Importance: 0.0\n"
          ]
        }
      ],
      "source": [
        "for feature_name, importance in sorted(zip(X.columns, drink_clf.feature_importances_), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"Feature: {feature_name}, Importance: {importance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train a RandomForestClassifier for drinking\n",
        "rf_drink = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=6)\n",
        "rf_drink.fit(X, y_drink)\n",
        "\n",
        "# Get feature importances for drinking\n",
        "feature_importances_drink = rf_drink.feature_importances_\n",
        "\n",
        "# Print or visualize feature importances for drinking\n",
        "for feature_name, importance in zip(X.columns, feature_importances_drink):\n",
        "    print(f\"Feature: {feature_name}, Importance: {importance}\")\n",
        "\n",
        "# Initialize and train a RandomForestClassifier for smoking\n",
        "rf_smoke = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=6)\n",
        "rf_smoke.fit(X, y_smoke)\n",
        "\n",
        "# Get feature importances for smoking\n",
        "feature_importances_smoke = rf_smoke.feature_importances_\n",
        "\n",
        "# Print or visualize feature importances for smoking\n",
        "for feature_name, importance in zip(X.columns, feature_importances_smoke):\n",
        "    print(f\"Feature: {feature_name}, Importance: {importance}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxkT4mdn79js",
        "outputId": "1e43326f-6b17-4fcc-f0ac-69be7f65443d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New features generated  462\n"
          ]
        }
      ],
      "source": [
        "non_linear_data = {}\n",
        "\n",
        "for c in X.columns:\n",
        "    for d in X.columns:\n",
        "        if c != d:\n",
        "            non_linear_data[f\"{c}_{d}\"] = X[c] * X[d]\n",
        "\n",
        "print(\"New features generated \", len(non_linear_data))\n",
        "\n",
        "non_linear_df = pd.DataFrame(non_linear_data, index=X.index)\n",
        "X = pd.concat([X, non_linear_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "8oDN7QEQM7tf",
        "outputId": "43bbec18-9cb1-44f5-da25-0d48b86e0ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature importances for DRK_YN:\n",
            "             Feature  Importance\n",
            "1                age    0.275584\n",
            "21         gamma_GTP    0.210984\n",
            "13         HDL_chole    0.148946\n",
            "0                sex    0.147854\n",
            "20          SGOT_ALT    0.114065\n",
            "2             height    0.040390\n",
            "10               DBP    0.016869\n",
            "14         LDL_chole    0.016004\n",
            "18  serum_creatinine    0.011505\n",
            "16        hemoglobin    0.008937\n",
            "15      triglyceride    0.008861\n",
            "5         sight_left    0.000000\n",
            "3             weight    0.000000\n",
            "4          waistline    0.000000\n",
            "12         tot_chole    0.000000\n",
            "11              BLDS    0.000000\n",
            "8         hear_right    0.000000\n",
            "9                SBP    0.000000\n",
            "7          hear_left    0.000000\n",
            "6        sight_right    0.000000\n",
            "17     urine_protein    0.000000\n",
            "19          SGOT_AST    0.000000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-35e6284c34b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Feature Importance for SMK_stat_type_cd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mada_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_smoke\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mfeature_importances_smoke\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mada_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \"\"\"\n\u001b[1;32m   1228\u001b[0m     \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Assume 'dataset' is your DataFrame already loaded\n",
        "# Separate features (X) and targets (y)\n",
        "X = dataset.drop(['DRK_YN', 'SMK_stat_type_cd'], axis=1)\n",
        "y_drink = dataset['DRK_YN']\n",
        "y_smoke = dataset['SMK_stat_type_cd']\n",
        "\n",
        "# Initialize AdaBoostClassifier with a DecisionTreeClassifier as base estimator\n",
        "ada_classifier = AdaBoostClassifier(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Feature Importance for DRK_YN\n",
        "# ---------------------------\n",
        "ada_classifier.fit(X, y_drink)\n",
        "feature_importances_drink = ada_classifier.feature_importances_\n",
        "\n",
        "# Create DataFrame for feature importances (DRK_YN)\n",
        "feature_importance_df_drink = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_drink\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature importances for DRK_YN:\")\n",
        "print(feature_importance_df_drink)\n",
        "\n",
        "# ---------------------------\n",
        "# Feature Importance for SMK_stat_type_cd\n",
        "# ---------------------------\n",
        "ada_classifier.fit(X, y_smoke)\n",
        "feature_importances_smoke = ada_classifier.feature_importances_\n",
        "\n",
        "# Create DataFrame for feature importances (SMK_stat_type_cd)\n",
        "feature_importance_df_smoke = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': feature_importances_smoke\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature importances for SMK_stat_type_cd:\")\n",
        "print(feature_importance_df_smoke)\n",
        "\n",
        "# Select features that have an importance > 0\n",
        "\n",
        "selected_features_drink = feature_importance_df_drink[feature_importance_df_drink['Importance'] > 0]['Feature'].tolist()\n",
        "selected_features_smoke = feature_importance_df_smoke[feature_importance_df_smoke['Importance'] > 0]['Feature'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCTeipNbRQCz"
      },
      "outputs": [],
      "source": [
        "selected_features_drink = [\n",
        "    'age',\n",
        "    'gamma_GTP',\n",
        "    'HDL_chole',\n",
        "    'sex',\n",
        "    'SGOT_ALT',\n",
        "    'height',\n",
        "    'DBP',\n",
        "    'LDL_chole',\n",
        "    'serum_creatinine',\n",
        "    'hemoglobin',\n",
        "    'triglyceride'\n",
        "]\n",
        "\n",
        "selected_features_smoke = [\n",
        "    # 'sex',\n",
        "    'age',\n",
        "    'gamma_GTP',\n",
        "    'weight',\n",
        "    'height',\n",
        "    'SGOT_AST',\n",
        "    'triglyceride',\n",
        "    'hemoglobin',\n",
        "    'serum_creatinine',\n",
        "    'HDL_chole',\n",
        "    'SGOT_ALT',\n",
        "    'LDL_chole',\n",
        "    'SBP'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-r5a5SzdONT",
        "outputId": "21c85f36-9045-41c0-9a94-ea218e279448"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:01<00:00,  7.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "121\n",
            "Accuracy: 0.7219599535986281\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "X = dataset[selected_features_drink].copy()\n",
        "y = dataset['DRK_YN'].copy()\n",
        "\n",
        "non_linear_data = {}\n",
        "\n",
        "for c in tqdm(X.columns):\n",
        "    for d in X.columns:\n",
        "        if c != d:\n",
        "            non_linear_data[f\"{c}_{d}\"] = X[c] * X[d]\n",
        "\n",
        "non_linear_df = pd.DataFrame(non_linear_data, index=X.index)\n",
        "X = pd.concat([X, non_linear_df], axis=1)\n",
        "\n",
        "print(len(X.columns))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Initialize and train the AdaBoost classifier\n",
        "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42) # Adjust n_estimators as needed\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example using accuracy)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# You can add other evaluation metrics like f1_score, precision_score, recall_score, confusion_matrix, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA8AEOfkK0HX",
        "outputId": "a5b6101e-6bfe-45a2-c010-168a9506ab7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:01<00:00,  8.85it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming 'dataset' is already loaded and 'selected_features_smoke' is defined\n",
        "# For this example, we'll use 'selected_features_smoke' as the features,\n",
        "# and 'SMK_stat_type_cd' as the target variable.\n",
        "\n",
        "# Prepare the data\n",
        "\n",
        "# X = dataset[selected_features_smoke].copy()\n",
        "# y = dataset['SMK_stat_type_cd'].copy()\n",
        "\n",
        "X = dataset[selected_features_drink].copy()\n",
        "y = dataset['DRK_YN'].copy()\n",
        "\n",
        "# create nonlinear columns from exiting ones\n",
        "\n",
        "non_linear_data = {}\n",
        "\n",
        "for c in tqdm(X.columns):\n",
        "    for d in X.columns:\n",
        "        if c != d:\n",
        "            non_linear_data[f\"{c}_{d}\"] = X[c] * X[d]\n",
        "\n",
        "non_linear_df = pd.DataFrame(non_linear_data, index=X.index)\n",
        "X = pd.concat([X, non_linear_df], axis=1)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = X.select_dtypes(include=['number']).columns\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train a LinearSVC model\n",
        "model = LinearSVC(random_state=42)  # Increase max_iter if needed\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using accuracy and other metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate a classification report that includes precision, recall, and F1-score\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zb-fj8xLHJl"
      },
      "outputs": [],
      "source": [
        "# prompt: use a svm to classify the DRK_YN column\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train an SVC model\n",
        "svm_model = SVC(kernel='rbf', random_state=42) # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
        "\n",
        "# Generate a classification report that includes precision, recall, and F1-score\n",
        "report = classification_report(y_test, svm_y_pred)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgsElvSC7D3I"
      },
      "outputs": [],
      "source": [
        "# prompt: apply encoding to categorical values\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "dataset['sex'] = le.fit_transform(dataset['sex'])\n",
        "dataset['SMK_stat_type_cd'] = le.fit_transform(dataset['SMK_stat_type_cd'])\n",
        "dataset['DRK_YN'] = le.fit_transform(dataset['DRK_YN'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAu_-WRk6mfE"
      },
      "outputs": [],
      "source": [
        "# prompt: apply adaboost to the dataset\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Assuming 'dataset' is your DataFrame and you've already preprocessed it\n",
        "# Example preprocessing (adapt to your needs):\n",
        "# Separate features (X) and target variable (y)\n",
        "# Encode categorical features if necessary\n",
        "# Scale numerical features if necessary\n",
        "\n",
        "X = dataset.drop('DRK_YN', axis=1) # Replace 'target_variable_column' with your actual target column name\n",
        "y = dataset['DRK_YN'] # Replace 'target_variable_column'\n",
        "\n",
        "# resize X and y to 10%\n",
        "# X = X.sample(frac=0.1, random_state=42)\n",
        "# y = y[X.index]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "\n",
        "# Initialize and train the AdaBoost classifier\n",
        "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42) # Adjust n_estimators as needed\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example using accuracy)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# You can add other evaluation metrics like f1_score, precision_score, recall_score, confusion_matrix, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afQ32ruN98eo"
      },
      "outputs": [],
      "source": [
        "X = dataset.drop('SMK_stat_type_cd', axis=1) # Replace 'target_variable_column' with your actual target column name\n",
        "X.drop('DRK_YN', axis=1, inplace=True)\n",
        "y = dataset['SMK_stat_type_cd'] # Replace 'target_variable_column'\n",
        "\n",
        "# resize X and y to 10%\n",
        "# X = X.sample(frac=0.1, random_state=42)\n",
        "# y = y[X.index]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "\n",
        "# Initialize and train the AdaBoost classifier\n",
        "ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42) # Adjust n_estimators as needed\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model (example using accuracy)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "083ga63Q5s_m"
      },
      "outputs": [],
      "source": [
        "# Analisi iniziale del dataset\n",
        "print(\"\\nPrime righe del dataset:\")\n",
        "print(dataset.head())\n",
        "\n",
        "print(\"\\nDimensioni del dataset:\")\n",
        "print(dataset.shape)\n",
        "\n",
        "print(\"\\nTipi di dati:\")\n",
        "print(dataset.info())\n",
        "\n",
        "print(\"\\nStatistiche descrittive di base:\")\n",
        "print(dataset.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEHFvjdD5s_m"
      },
      "source": [
        "__Prime considerazioni__: I dati sembrano non contenere dati nulli o NaN, però si osserva facilmente che per alcune feature ci sono dei dati che sembrano essere irrealistici (Es.: per la feature **waistline** la media risulta essere _81_, il 4° quartile _87_ e il valore massimo _999_); quindi nonostante la non presenza di valori nulli, bisogna verificare se sono presenti valori mancanti codificati in altro modo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya30wv4u5s_m"
      },
      "outputs": [],
      "source": [
        "smoke = dataset[\"SMK_stat_type_cd\"].value_counts()\n",
        "drink = dataset[\"DRK_YN\"].value_counts()\n",
        "\n",
        "# Distribuzione SMK_stat_type_cd\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1) # Num righe, num colonne, posizione\n",
        "sns.barplot(x=smoke.index, y=smoke.values, palette=\"Blues_d\")\n",
        "plt.title(\"Distribuzione delle classi relative al fumo\")\n",
        "plt.xlabel(\"Tipo di fumatore\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "\n",
        "# Distribuzione DRK_YN\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=drink.index, y=drink.values, palette=\"Greens_d\")\n",
        "plt.title(\"Distribuzione delle classi relative al bere\")\n",
        "plt.xlabel(\"Bevitore (Y/N)\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVnAIL_k5s_n"
      },
      "outputs": [],
      "source": [
        "dataset_mod = dataset.drop(columns=[\"sex\", \"SMK_stat_type_cd\", \"DRK_YN\"])\n",
        "\n",
        "# Boxplot per le principali feature numeriche\n",
        "plt.figure(figsize=(25, 20))\n",
        "for i, col in enumerate(dataset_mod.columns):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    sns.boxplot(y=dataset_mod[col], color=\"skyblue\")\n",
        "    plt.title(col)\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkmtsluJ5s_n"
      },
      "outputs": [],
      "source": [
        "# Verifica dei valori massimi e distanza dal secondo massimo e dalla media\n",
        "\n",
        "for col in dataset_mod.columns:\n",
        "    max = dataset_mod[col].max()\n",
        "    max_count = (dataset_mod[col] == max).sum()\n",
        "    second_max = dataset_mod[col][dataset_mod[col] < max].max()\n",
        "    distance_max = max - second_max\n",
        "    mean_value = dataset[col].mean()\n",
        "    distance_mean = max - mean_value\n",
        "\n",
        "    print(f\"Colonna: {col}\")\n",
        "    print(f\"    -Valore massimo: {max}\")\n",
        "    print(f\"    -Occorrenze del massimo: {max_count}\")\n",
        "    print(f\"    -Secondo massimo: {second_max}\")\n",
        "    print(f\"    -Distanza tra massimo e secondo massimo: {distance_max}\")\n",
        "    print(f\"    -Media: {mean_value}\")\n",
        "    print(f\"    -Distanza tra massimo e media: {distance_mean}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eMz189u5s_o"
      },
      "outputs": [],
      "source": [
        "# Determinazione del numero di valori \"fuori scala\" (outliers) per ogni feature\n",
        "\n",
        "for col in dataset_mod.columns:\n",
        "    Q1 = dataset_mod[col].quantile(0.25)\n",
        "    Q3 = dataset_mod[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = dataset_mod[(dataset_mod[col] < lower_bound) or (dataset_mod[col] > upper_bound)]\n",
        "    print(f\"Colonna: {col}\")\n",
        "    print(f\"    -Valori fuori scala: {len(outliers)}\")\n",
        "    print(f\"    -Limiti: {lower_bound} - {upper_bound}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KDYtrnn5s_o"
      },
      "source": [
        "### Considerazioni\n",
        "\n",
        "Un'analisi più approfondita ha permesso di individuare la presenza di alcuni outlier in specifiche feature, in particolare confrontando il valore massimo presente nel dataset con il secondo valore massimo per ciascuna di esse; è importante sottolineare però che non tutte le feature mostrano grosse differenze tra il primo e il secondo massimo ed in molti casi sono presenti molti valori tra il massimo e la media.\n",
        "\n",
        "Sono state fatte dunque delle brevi ricerche per capire quali valori fossero plausibili per ogni feature e quali no, è stato deciso di stabilire una soglia massima accettabile per alcune di queste; queste soglie sono state definite con l'obiettivo di preservare il maggior numero possibile di righe del dataset originario ma eliminando allo stesso tempo i casi clinicamente estremi e rari, in modo da ottenere un dataset più generico, privo di situazioni patologiche estreme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEbRdRj55s_p"
      },
      "outputs": [],
      "source": [
        "thresholds = {\n",
        "    \"waistline\": 200,\n",
        "    \"sight_left\": 4,\n",
        "    \"sight_right\": 4,\n",
        "    \"SBP\": 240,\n",
        "    \"DBP\": 160,\n",
        "    \"BLDS\": 600,\n",
        "    \"tot_chole\": 1000,\n",
        "    \"HDL_chole\": 700,\n",
        "    \"LDL_chole\": 2000,\n",
        "    \"triglyceride\": 3500,\n",
        "    \"serum_creatinine\": 30,\n",
        "    \"SGOT_AST\": 2000,\n",
        "    \"SGOT_ALT\": 2000,\n",
        "    \"gamma_GTP\": 900,\n",
        "}\n",
        "\n",
        "for col, threshold in thresholds.items():\n",
        "    dataset.loc[dataset[col] > threshold, col] = None\n",
        "\n",
        "# Rimozione delle righe con valori mancanti\n",
        "dataset_cleaned = dataset.dropna(subset=thresholds.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nbhaUfr5s_p"
      },
      "source": [
        "Ora verranno rieseguite tutte le operazioni di visualizzazione dei dati presenti nel dataset per comprendere la distribuzione di questi dopo l'operazione di rimozione degli outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrVQvTti5s_p"
      },
      "outputs": [],
      "source": [
        "# Analisi iniziale del dataset\n",
        "print(\"\\nPrime righe del dataset:\")\n",
        "print(dataset_cleaned.head())\n",
        "\n",
        "print(\"\\nDimensioni del dataset:\")\n",
        "print(dataset_cleaned.shape)\n",
        "\n",
        "print(\"\\nTipi di dati:\")\n",
        "print(dataset_cleaned.info())\n",
        "\n",
        "print(\"\\nStatistiche descrittive di base:\")\n",
        "print(dataset_cleaned.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUkOqNuX5s_q"
      },
      "outputs": [],
      "source": [
        "smoke = dataset_cleaned[\"SMK_stat_type_cd\"].value_counts()\n",
        "drink = dataset_cleaned[\"DRK_YN\"].value_counts()\n",
        "\n",
        "# Distribuzione SMK_stat_type_cd\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1) # Num righe, num colonne, posizione\n",
        "sns.barplot(x=smoke.index, y=smoke.values, palette=\"Blues_d\")\n",
        "plt.title(\"Distribuzione delle classi relative al fumo\")\n",
        "plt.xlabel(\"Tipo di fumatore\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "\n",
        "# Distribuzione DRK_YN\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=drink.index, y=drink.values, palette=\"Greens_d\")\n",
        "plt.title(\"Distribuzione delle classi relative al bere\")\n",
        "plt.xlabel(\"Bevitore (Y/N)\")\n",
        "plt.ylabel(\"Frequenza\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z0C6akF5s_q"
      },
      "outputs": [],
      "source": [
        "dataset_mod = dataset_cleaned.drop(columns=[\"sex\", \"SMK_stat_type_cd\", \"DRK_YN\"])\n",
        "\n",
        "# Boxplot per le principali feature numeriche\n",
        "plt.figure(figsize=(25, 20))\n",
        "for i, col in enumerate(dataset_mod.columns):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    sns.boxplot(y=dataset_mod[col], color=\"skyblue\")\n",
        "    plt.title(col)\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eUs1MMC5s_q"
      },
      "outputs": [],
      "source": [
        "# Verifica dei valori massimi e distanza dal secondo massimo e dalla media\n",
        "\n",
        "for col in dataset_mod.columns:\n",
        "    max = dataset_mod[col].max()\n",
        "    max_count = (dataset_mod[col] == max).sum()\n",
        "    second_max = dataset_mod[col][dataset_mod[col] < max].max()\n",
        "    distance_max = max - second_max\n",
        "    mean_value = dataset[col].mean()\n",
        "    distance_mean = max - mean_value\n",
        "\n",
        "    print(f\"Colonna: {col}\")\n",
        "    print(f\"    -Valore massimo: {max}\")\n",
        "    print(f\"    -Occorrenze del massimo: {max_count}\")\n",
        "    print(f\"    -Secondo massimo: {second_max}\")\n",
        "    print(f\"    -Distanza tra massimo e secondo massimo: {distance_max}\")\n",
        "    print(f\"    -Media: {mean_value}\")\n",
        "    print(f\"    -Distanza tra massimo e media: {distance_mean}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyKxMwmG5s_r"
      },
      "outputs": [],
      "source": [
        "# Determinazione del numero di valori \"fuori scala\" (outliers) per ogni feature\n",
        "\n",
        "for col in dataset_mod.columns:\n",
        "    Q1 = dataset_mod[col].quantile(0.25)\n",
        "    Q3 = dataset_mod[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = dataset_mod[(dataset_mod[col] < lower_bound) or (dataset_mod[col] > upper_bound)]\n",
        "    print(f\"Colonna: {col}\")\n",
        "    print(f\"    -Valori fuori scala: {len(outliers)}\")\n",
        "    print(f\"    -Limiti: {lower_bound} - {upper_bound}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfZVHaiv5s_r"
      },
      "source": [
        "Una volta terminata la fase di analisi e pulizia dei dati, sarebbe opportuno procedere con le operazioni di encoding e scaling dei dati (visto che sono presenti features con valori molto grandi e altre con valori più piccoli), però pensiamo sia più opportuno fare queste operazioni eventualmente in un secondo momento a seconda dei modelli di ML che decidiamo di utilizzare.\n",
        "Per ora ci limitiamo a fare l'encoding delle feature categoriali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZoCtMV_5s_r"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [\"sex\", \"DRK_YN\"]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    encoder = LabelEncoder()\n",
        "    dataset_cleaned[col] = encoder.fit_transform(dataset_cleaned[col])\n",
        "\n",
        "print(dataset_cleaned.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNfFtKVJ5s_s"
      },
      "source": [
        "### Un primo esempio\n",
        "\n",
        "Di seguito riportiamo un semplice modello di albero decisionale (o classificatore naive di Bayes?) per vedere che lower bound abbiamo per quanto riguarda le metriche di valutazione (accuratezza, recall, precisione, f1_score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O5BX8--5s_s"
      },
      "outputs": [],
      "source": [
        "# Divisione del dataset in feature e target (X e Y)\n",
        "target_smoke = \"SMK_stat_type_cd\"\n",
        "target_drink = \"DRK_YN\"\n",
        "#X_smoke = dataset_cleaned.iloc[:, dataset_cleaned.columns != target_smoke]\n",
        "X_smoke = dataset_cleaned.drop(columns=[\"SMK_stat_type_cd\", \"DRK_YN\"])\n",
        "Y_smoke = dataset_cleaned[target_smoke]\n",
        "#X_drink = dataset_cleaned.iloc[:, dataset_cleaned.columns != target_drink]\n",
        "X_drink = dataset_cleaned.drop(columns=[\"SMK_stat_type_cd\", \"DRK_YN\"])\n",
        "Y_drink = dataset_cleaned[target_drink]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S44oM10r5s_s"
      },
      "outputs": [],
      "source": [
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_smoke, Y_smoke, test_size=0.3, random_state=42)\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train_s, y_train_s)\n",
        "y_pred = tree.predict(X_test_s)\n",
        "accuracy = accuracy_score(y_test_s, y_pred) # balanced_accuracy_score(y_test_s, y_pred)\n",
        "precision = precision_score(y_test_s, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test_s, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test_s, y_pred, average=\"weighted\")\n",
        "print(f\"Accuratezza: {accuracy*100:.2f}%\")\n",
        "print(f\"Precisione: {precision*100:.2f}%\")\n",
        "print(f\"Recall: {recall*100:.2f}%\")\n",
        "print(f\"F1: {f1*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LepSj295s_s"
      },
      "outputs": [],
      "source": [
        "# Matrice di confusione per il fumo\n",
        "confusion_matrix_smoking = confusion_matrix(y_test_s, y_pred)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_smoking, display_labels=tree.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "display.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.title(\"Matrice di confusione - Fumo\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unGEE0SL5s_t"
      },
      "outputs": [],
      "source": [
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_drink, Y_drink, test_size=0.3, random_state=42)\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train_d, y_train_d)\n",
        "y_pred = tree.predict(X_test_d)\n",
        "accuracy = accuracy_score(y_test_d, y_pred)\n",
        "precision = precision_score(y_test_d, y_pred)\n",
        "recall = recall_score(y_test_d, y_pred)\n",
        "f1 = f1_score(y_test_d, y_pred)\n",
        "print(f\"Accuratezza: {accuracy*100:.2f}%\")\n",
        "print(f\"Precisione: {precision*100:.2f}%\")\n",
        "print(f\"Recall: {recall*100:.2f}%\")\n",
        "print(f\"F1: {f1*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id1LjlRW5s_t"
      },
      "outputs": [],
      "source": [
        "# Matrice di confusione per il bere\n",
        "confusion_matrix_drinking = confusion_matrix(y_test_d, y_pred)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_drinking, display_labels=tree.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "display.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.title(\"Matrice di confusione - Fumo\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWW0NLT15s_t"
      },
      "source": [
        "__Prossimi passi__: da adesso in poi l'obiettivo sarà migliorare sempre di più le metriche delle predizioni sia per il caso del fumo che del bere, utilizzando modelli di apprendimento diversi e sfruttando tecniche di normalizzazione, feature selection, dimensionality reduction, encoding, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-0u9AC5s_t"
      },
      "source": [
        "### RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEbem_nC5s_t"
      },
      "outputs": [],
      "source": [
        "# Iniziamo ad utilizzare dei modelli un po' più complessi e poco alla volta andiamo a migliorare il nostro modello con tecniche di preprocessing e tuning dei parametri\n",
        "# Partiamo da una RandomForest (da adesso in poi le predizioni e le metriche, verranno salvate in nomi di variabili che contengono il nome del modello e la lettera 's' per smoke e 'd' per drink)\n",
        "\n",
        "forest = RandomForestClassifier(random_state=42, class_weight=\"balanced\")\n",
        "forest.fit(X_train_s, y_train_s)\n",
        "y_pred_rf_s = forest.predict(X_test_s)\n",
        "accuracy_rf_s = accuracy_score(y_test_s, y_pred_rf_s) # balanced_accuracy_score(y_test_s, y_pred)\n",
        "precision_rf_s = precision_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "recall_rf_s = recall_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "f1_rf_s = f1_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "print(f\"Accuratezza: {accuracy_rf_s*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_rf_s*100:.2f}%\")\n",
        "print(f\"Recall: {recall_rf_s*100:.2f}%\")\n",
        "print(f\"F1: {f1_rf_s*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-_XKi585s_u"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(random_state=42)\n",
        "forest.fit(X_train_d, y_train_d)\n",
        "y_pred_rf_d = forest.predict(X_test_d)\n",
        "accuracy_rf_d = accuracy_score(y_test_d, y_pred_rf_d)\n",
        "precision_rf_d = precision_score(y_test_d, y_pred_rf_d)\n",
        "recall_rf_d = recall_score(y_test_d, y_pred_rf_d)\n",
        "f1_rf_d = f1_score(y_test_d, y_pred_rf_d)\n",
        "print(f\"Accuratezza: {accuracy_rf_d*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_rf_d*100:.2f}%\")\n",
        "print(f\"Recall: {recall_rf_d*100:.2f}%\")\n",
        "print(f\"F1: {f1_rf_d*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65AavrcA5s_u"
      },
      "outputs": [],
      "source": [
        "# Procediamo con il selezionare un paio di parametri e combinarli tra loro per vedere quale modello ottiene i risultati migliori\n",
        "\n",
        "parameters = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [5, 10, 20],\n",
        "    \"min_samples_split\": [50, 100, 200]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm06YgpH5s_u"
      },
      "outputs": [],
      "source": [
        "for n_estimators in parameters[\"n_estimators\"]:\n",
        "    for max_depth in parameters[\"max_depth\"]:\n",
        "        for min_samples_split in parameters[\"min_samples_split\"]:\n",
        "            forest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42, class_weight=\"balanced\")\n",
        "            forest.fit(X_train_s, y_train_s)\n",
        "            y_pred_rf_s = forest.predict(X_test_s)\n",
        "            accuracy_rf_s = accuracy_score(y_test_s, y_pred_rf_s) # balanced_accuracy_score(y_test_s, y_pred)\n",
        "            precision_rf_s = precision_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "            recall_rf_s = recall_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "            f1_rf_s = f1_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "            print(f\"n_estimators: {n_estimators}, max_depth: {max_depth}, min_samples_split: {min_samples_split}\")\n",
        "            print(f\"    Accuratezza: {accuracy_rf_s*100:.2f}%\")\n",
        "            print(f\"    Precisione: {precision_rf_s*100:.2f}%\")\n",
        "            print(f\"    Recall: {recall_rf_s*100:.2f}%\")\n",
        "            print(f\"    F1: {f1_rf_s*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_N3GN4d5s_u"
      },
      "source": [
        "La miglior scelta dei parametri risulta essere la seguente:\n",
        "- **n_estimators**: 100\n",
        "- **max_depth**: 20\n",
        "- **min_samples_split**: 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quk9_y8P5s_u"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42, class_weight=\"balanced\")\n",
        "forest.fit(X_train_s, y_train_s)\n",
        "y_pred_rf_s = forest.predict(X_test_s)\n",
        "accuracy_rf_s = accuracy_score(y_test_s, y_pred_rf_s) # balanced_accuracy_score(y_test_s, y_pred)\n",
        "precision_rf_s = precision_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "recall_rf_s = recall_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "f1_rf_s = f1_score(y_test_s, y_pred_rf_s, average=\"weighted\")\n",
        "print(f\"Accuratezza: {accuracy_rf_s*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_rf_s*100:.2f}%\")\n",
        "print(f\"Recall: {recall_rf_s*100:.2f}%\")\n",
        "print(f\"F1: {f1_rf_s*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPrluUb-5s_v"
      },
      "outputs": [],
      "source": [
        "for n_estimators in parameters[\"n_estimators\"]:\n",
        "    for max_depth in parameters[\"max_depth\"]:\n",
        "        for min_samples_split in parameters[\"min_samples_split\"]:\n",
        "            forest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
        "            forest.fit(X_train_d, y_train_d)\n",
        "            y_pred_rf_d = forest.predict(X_test_d)\n",
        "            accuracy_rf_d = accuracy_score(y_test_d, y_pred_rf_d)\n",
        "            precision_rf_d = precision_score(y_test_d, y_pred_rf_d)\n",
        "            recall_rf_d = recall_score(y_test_d, y_pred_rf_d)\n",
        "            f1_rf_d = f1_score(y_test_d, y_pred_rf_d)\n",
        "            print(f\"n_estimators: {n_estimators}, max_depth: {max_depth}, min_samples_split: {min_samples_split}\")\n",
        "            print(f\"    Accuratezza: {accuracy_rf_d*100:.2f}%\")\n",
        "            print(f\"    Precisione: {precision_rf_d*100:.2f}%\")\n",
        "            print(f\"    Recall: {recall_rf_d*100:.2f}%\")\n",
        "            print(f\"    F1: {f1_rf_d*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbIRPtq15s_v"
      },
      "source": [
        "La miglior scelta dei parametri risulta essere la seguente:\n",
        "- **n_estimators**: 100\n",
        "- **max_depth**: 20\n",
        "- **min_samples_split**: 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7oLZV0h5s_v"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42)\n",
        "forest.fit(X_train_d, y_train_d)\n",
        "y_pred_rf_d = forest.predict(X_test_d)\n",
        "accuracy_rf_d = accuracy_score(y_test_d, y_pred_rf_d)\n",
        "precision_rf_d = precision_score(y_test_d, y_pred_rf_d)\n",
        "recall_rf_d = recall_score(y_test_d, y_pred_rf_d)\n",
        "f1_rf_d = f1_score(y_test_d, y_pred_rf_d)\n",
        "print(f\"Accuratezza: {accuracy_rf_d*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_rf_d*100:.2f}%\")\n",
        "print(f\"Recall: {recall_rf_d*100:.2f}%\")\n",
        "print(f\"F1: {f1_rf_d*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLBlN2q15s_v"
      },
      "source": [
        "Ora che sono stati definiti i parametri migliori per le random forest, procediamo con la tecnica della feature selection con la speranza di migliorare i risultati di accuratezza, precisione, recall e f1_score; pensiamo sia un'operazione da compiere per poter ottenere risultati migliori, perchè abbiamo a che fare con un dataset con 22 feature che non sono poche e il rischio di overfit c'è. Probabilmente con una random forest questo rischio è minore, però testiamo se anche con una random forest i risultati migliorano."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inCHNV_V5s_v"
      },
      "source": [
        "### RandomForest con feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxOJPW8Z5s_8"
      },
      "source": [
        "In questa fase del progetto, proviamo ad ottimizzare la RandomForest facendo feature selection; avendo a che fare con un modello predittivo di questo tipo, ovvero abbastanza robusto alle feature irrilevanti, non ci aspettiamo un grosso miglioramento delle performance ma nonostante ciò è comunque un passaggio utile per poter eventualmente semplificare il modello andando a togliere anche poche feature e rendere il modello predittivo poco più veloce.\n",
        "Inoltre nella prossima cella di codice, non viene indicato a priori un numero fissato di feature da selezionare, in quanto non certi di quelle che possano essere le performance selezionando solo il 10%, il 20%, il 50%, il 75%... delle feature; quindi calcoliamo la f1_score (che per il caso multiclasse ci sembra essere la metrica migliore) ad ogni best_feature aggiunta e salviamo la combinazione di feature migliore tra tutte quelle testate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFAdkyA75s_8"
      },
      "outputs": [],
      "source": [
        "X_train_s, X_val_s, y_train_s, y_val_s = train_test_split(X_smoke, Y_smoke, test_size=0.3, random_state=42)\n",
        "\n",
        "selected_features_s = []\n",
        "best_selected_features_s = []\n",
        "remaining_features = [col for col in X_train_s.columns]\n",
        "best_overall_score = 0\n",
        "\n",
        "while len(remaining_features) > 0:\n",
        "    best_score = 0\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in remaining_features:\n",
        "        current_features = selected_features_s + [feature]\n",
        "        forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42, class_weight=\"balanced\")\n",
        "        forest.fit(X_train_s.loc[:, current_features], y_train_s)\n",
        "        y_pred = forest.predict(X_val_s.loc[:, current_features])\n",
        "        score = f1_score(y_val_s, y_pred, average=\"weighted\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_feature = feature\n",
        "\n",
        "    selected_features_s.append(best_feature)\n",
        "    remaining_features.remove(best_feature)\n",
        "    print(f\"Feature selezionata: {best_feature}, score: {best_score*100:.2f}%\")\n",
        "\n",
        "    if best_score > best_overall_score:\n",
        "        best_overall_score = score\n",
        "        best_selected_features_s = selected_features_s.copy()\n",
        "\n",
        "print(\"Feature finali selezionate:\", best_selected_features_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyfB2ZHw5s_8"
      },
      "source": [
        "**NB:** i risultati della cella precedente sono stati copiati e riportati in questa cella di markdown perchè i tempi per ottenere i risultati erano molto lunghi e sono stati ottenuti in momenti diversi.\n",
        "\n",
        "Feature selezionata: height, score: 61.46%  \n",
        "Feature selezionata: sex, score: 66.84%  \n",
        "Feature selezionata: age, score: 69.13%  \n",
        "Feature selezionata: weight, score: 69.33%  \n",
        "Feature selezionata: gamma_GTP, score: 69.35%  \n",
        "Feature selezionata: HDL_chole, score: 69.77%  \n",
        "Feature selezionata: SGOT_ALT, score: 70.01%  \n",
        "Feature selezionata: hemoglobin, score: 70.16%  \n",
        "Feature selezionata: SGOT_AST, score: 70.24%  \n",
        "Feature selezionata: DBP, score: 70.30%  \n",
        "Feature selezionata: LDL_chole, score: 70.32%  \n",
        "Feature selezionata: serum_creatinine, score: 70.36%  \n",
        "Feature selezionata: triglyceride, score: 70.40%  \n",
        "Feature selezionata: BLDS, score: 70.42%  \n",
        "Feature selezionata: sight_right, score: 70.41%  \n",
        "Feature selezionata: hear_right, score: 70.43%  \n",
        "Feature selezionata: urine_protein, score: 70.42%  \n",
        "Feature selezionata: SBP, score: 70.40%  \n",
        "Feature selezionata: waistline, score: 70.39%  \n",
        "Feature selezionata: hear_left, score: 70.42%  \n",
        "Feature selezionata: tot_chole, score: 70.38%  \n",
        "Feature selezionata: urine_protein, score: 70.37%  \n",
        "Feature selezionata: sight_left, score: 70.40%  \n",
        "Feature selezionata: SBP, score: 70.35%  \n",
        "Feature finali selezionate: ['age', 'sex', 'height', 'weight', 'hear_right', 'sight_right', 'DBP', 'BLDS', 'HDL_chole', 'LDL_chole', 'triglyceride', 'serum_creatinine', 'SGOT_AST', 'SGOT_ALT', 'gamma_GTP', 'hemoglobin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HDLskSX5s_8"
      },
      "outputs": [],
      "source": [
        "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_drink, Y_drink, test_size=0.3, random_state=42)\n",
        "\n",
        "selected_features_d = []\n",
        "best_selected_features_d = []\n",
        "remaining_features = [col for col in X_train_d.columns]\n",
        "best_overall_score = 0\n",
        "\n",
        "while len(remaining_features) > 0:\n",
        "    best_score = 0\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in remaining_features:\n",
        "        current_features = selected_features_d + [feature]\n",
        "        forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42)\n",
        "        forest.fit(X_train_d.loc[:, current_features], y_train_d)\n",
        "        y_pred = forest.predict(X_val_d.loc[:, current_features])\n",
        "        score = accuracy_score(y_val_d, y_pred)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_feature = feature\n",
        "\n",
        "    selected_features_d.append(best_feature)\n",
        "    remaining_features.remove(best_feature)\n",
        "    print(f\"Feature selezionata: {best_feature}, score: {best_score*100:.2f}%\")\n",
        "\n",
        "    if best_score > best_overall_score:\n",
        "        best_overall_score = score\n",
        "        best_selected_features_d = selected_features_d.copy()\n",
        "\n",
        "print(\"Feature finali selezionate:\", best_selected_features_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXejwIzt5s_9"
      },
      "source": [
        "**NB:** i risultati della cella precedente sono stati copiati e riportati in questa cella di markdown perchè i tempi per ottenere i risultati erano molto lunghi e sono stati ottenuti in momenti diversi.\n",
        "\n",
        "Feature selezionata: sex, score: 68.30%  \n",
        "Feature selezionata: age, score: 70.13%  \n",
        "Feature selezionata: gamma_GTP, score: 71.26%   \n",
        "Feature selezionata: HDL_chole, score: 71.61%  \n",
        "Feature selezionata: SGOT_ALT, score: 72.25%  \n",
        "Feature selezionata: weight, score: 72.53%  \n",
        "Feature selezionata: SGOT_AST, score: 72.74%  \n",
        "Feature selezionata: tot_chole, score: 72.90%  \n",
        "Feature selezionata: triglyceride, score: 72.94%  \n",
        "Feature selezionata: LDL_chole, score: 73.01%  \n",
        "Feature selezionata: serum_creatinine, score: 72.99%  \n",
        "Feature selezionata: DBP, score: 73.06%  \n",
        "Feature selezionata: hemoglobin, score: 73.05%  \n",
        "Feature selezionata: BLDS, score: 73.04%  \n",
        "Feature selezionata: hear_right, score: 72.99%  \n",
        "Feature selezionata: waistline, score: 73.06%  \n",
        "Feature selezionata: hear_left, score: 73.03%  \n",
        "Feature selezionata: height, score: 73.07%  \n",
        "Feature selezionata: sight_left, score: 73.05%  \n",
        "Feature selezionata: sight_right, score: 73.01%  \n",
        "Feature selezionata: SBP, score: 72.97%  \n",
        "Feature selezionata: urine_protein, score: 72.90%  \n",
        "Feature finali selezionate: ['sex', 'age', 'gamma_GTP', 'HDL_chole', 'SGOT_ALT', 'weight', 'SGOT_AST', 'tot_chole', 'triglyceride', 'LDL_chole', 'serum_creatinine', 'DBP', 'hemoglobin', 'BLDS', 'hear_right', 'waistline', 'hear_left', 'height']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDX2wlh45s_9"
      },
      "outputs": [],
      "source": [
        "best_selected_features_d = [\"sex\", \"age\", \"gamma_GTP\", \"HDL_chole\", \"SGOT_ALT\", \"weight\", \"SGOT_AST\", \"tot_chole\", \"triglyceride\", \"LDL_chole\", \"serum_creatinine\", \"DBP\", \"hemoglobin\", \"BLDS\", \"hear_right\", \"waistline\", \"hear_left\", \"height\"]\n",
        "best_selected_features_s = [\"age\", \"sex\", \"height\", \"weight\", \"hear_right\", \"sight_right\", \"DBP\", \"BLDS\", \"HDL_chole\", \"LDL_chole\", \"triglyceride\", \"serum_creatinine\", \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\", \"hemoglobin\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peoef1Ok5s_9"
      },
      "outputs": [],
      "source": [
        "# Matrice di confusione per il fumo e con le feature selezionate\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42, class_weight=\"balanced\")\n",
        "forest.fit(X_train_s.loc[:, best_selected_features_s], y_train_s)\n",
        "y_pred_rf_s = forest.predict(X_test_s.loc[:, best_selected_features_s])\n",
        "confusion_matrix_s = confusion_matrix(y_test_s, y_pred_rf_s)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_s, display_labels=forest.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "display.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.title(\"Matrice di confusione - Fumo\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr_HvVHw5s_9"
      },
      "outputs": [],
      "source": [
        "# Matrice di confusione per il bere e con le feature selezionate\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42)\n",
        "forest.fit(X_train_d.loc[:, best_selected_features_d], y_train_d)\n",
        "y_pred_rf_d = forest.predict(X_test_d.loc[:, best_selected_features_d])\n",
        "confusion_matrix_d = confusion_matrix(y_test_d, y_pred_rf_d)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_d, display_labels=forest.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "display.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.title(\"Matrice di confusione - bere\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUu2esaI5s_-"
      },
      "source": [
        "### Random forest con feature reduction e cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewPH2MEZ5s_-"
      },
      "source": [
        "In questa ultima parte relativa l'utilizzo della RandomForest come modello di appredimento, useremo la cross validation per valutare in maniera più precisa le performance dopo aver fatto feature selection.\n",
        "Siccome nello step precedente abbiamo allenato il nostro modello tenendo però costante il set di train e di validazione, potremmo aver \"overfittato\" in base alla suddivisione specifica; con la cross-validation, testiamo il modello su diverse porzioni del dataset e possiamo valutare se le feature scelte migliorano davvero il modello.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xbKvOsz5s_-"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42, class_weight=\"balanced\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_score_rf_s = cross_val_score(forest, X_smoke.loc[:, best_selected_features_s], Y_smoke, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
        "accuracy_rf_s = cross_val_score(forest, X_smoke.loc[:, best_selected_features_s], Y_smoke, cv=cv, scoring='balanced_accuracy', n_jobs=-1)\n",
        "print(f\"F1-score medio: {f1_score_rf_s.mean()*100:.2f}%\")\n",
        "print(f\"Accuratezza media: {accuracy_rf_s.mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL3AoELS5s_-"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_split=50, random_state=42)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "f1_score_rf_d = cross_val_score(forest, X_drink.loc[:, best_selected_features_d], Y_drink, cv=cv, scoring='f1', n_jobs=-1)\n",
        "accuracy_rf_d = cross_val_score(forest, X_drink.loc[:, best_selected_features_d], Y_drink, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"F1-score medio: {f1_score_rf_d.mean()*100:.2f}%\")\n",
        "print(f\"Accuratezza media: {accuracy_rf_d.mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zClm9A--5s_-"
      },
      "source": [
        "Prossimi modelli da poter usare:\n",
        "- KNN\n",
        "- SVM\n",
        "- AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TWCR64h5s__"
      },
      "source": [
        "### Support Vector Machine\n",
        "\n",
        "Da adesso in poi, invece, verrà utilizzato un modello di predizione diverso e di conseguenza verranno anche utilizzate tecniche di scaling e encoding per sfruttare al meglio le caratteristiche del modello stesso.  \n",
        "Più nel dettaglio verrà fatto:\n",
        "- __Scaling delle feature:__ SVM è sensibile alle diverse scale dei dati, quindi dobbiamo standardizzare le feature numeriche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvyxCqZZ5s__"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_smoke_scaled = scaler.fit_transform(X_smoke)\n",
        "X_drink_scaled = scaler.fit_transform(X_drink)\n",
        "X_smoke_scaled = pd.DataFrame(X_smoke_scaled, columns=X_smoke.columns)\n",
        "X_drink_scaled = pd.DataFrame(X_drink_scaled, columns=X_drink.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWoxBTwS5s__"
      },
      "outputs": [],
      "source": [
        "# Siccome il train di un modello SVM è molto pesante, utilizziamo solo una parte delle feature\n",
        "best_selected_features_d = [\"sex\", \"age\", \"gamma_GTP\", \"HDL_chole\", \"SGOT_ALT\", \"weight\", \"SGOT_AST\", \"tot_chole\", \"triglyceride\", \"LDL_chole\", \"serum_creatinine\", \"DBP\"]\n",
        "best_selected_features_s = [\"age\", \"sex\", \"height\", \"weight\", \"DBP\", \"BLDS\", \"HDL_chole\", \"LDL_chole\", \"triglyceride\", \"serum_creatinine\", \"SGOT_AST\", \"SGOT_ALT\", \"gamma_GTP\", \"hemoglobin\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgYe_nVf5s__"
      },
      "outputs": [],
      "source": [
        "# Prova con SVM con kernel lineare e train_set ridotto (34 minuti per ottenere i risultati)\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_smoke_scaled, Y_smoke, test_size=0.8, random_state=42)\n",
        "svm = SVC(kernel=\"linear\", class_weight=\"balanced\", C=1)\n",
        "svm.fit(X_train_s.loc[:, best_selected_features_s], y_train_s)\n",
        "y_pred = svm.predict(X_test_s.loc[:,best_selected_features_s])\n",
        "accuracy_svm_s = accuracy_score(y_test_s, y_pred) # balanced_accuracy_score(y_test_s, y_pred)\n",
        "precision_svm_s = precision_score(y_test_s, y_pred, average=\"weighted\")\n",
        "recall_svm_s = recall_score(y_test_s, y_pred, average=\"weighted\")\n",
        "f1_svm_s = f1_score(y_test_s, y_pred, average=\"weighted\")\n",
        "print(f\"Accuratezza: {accuracy_svm_s*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_svm_s*100:.2f}%\")\n",
        "print(f\"Recall: {recall_svm_s*100:.2f}%\")\n",
        "print(f\"F1: {f1_svm_s*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoC0WpV15s__"
      },
      "outputs": [],
      "source": [
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_drink_scaled, Y_drink, test_size=0.3, random_state=42)\n",
        "svm = SVC(kernel=\"linear\", C=1)\n",
        "svm.fit(X_train_d, y_train_d)\n",
        "y_pred = svm.predict(X_test_d)\n",
        "accuracy_svm_d = accuracy_score(y_test_d, y_pred)\n",
        "precision_svm_d = precision_score(y_test_d, y_pred)\n",
        "recall_svm_d = recall_score(y_test_d, y_pred)\n",
        "f1_svm_d = f1_score(y_test_d, y_pred)\n",
        "print(f\"Accuratezza: {accuracy_svm_d*100:.2f}%\")\n",
        "print(f\"Precisione: {precision_svm_d*100:.2f}%\")\n",
        "print(f\"Recall: {recall_svm_d*100:.2f}%\")\n",
        "print(f\"F1: {f1_svm_d*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (univr-ml)",
      "language": "python",
      "name": "univr-ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
